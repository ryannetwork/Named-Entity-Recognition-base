{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 LSTM 进行命名实体识别\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "from os.path import join\n",
    "from codecs import open\n",
    "\n",
    "import torch\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(split, make_vocab=True, data_dir=\"./ResumeNER\"):\n",
    "    \"\"\"读取数据\"\"\"\n",
    "    assert split in ['train', 'dev', 'test']\n",
    "\n",
    "    word_lists = []\n",
    "    tag_lists = []\n",
    "    with open(join(data_dir, split+\".char.bmes\"), 'r', encoding='utf-8') as f:\n",
    "        word_list = []\n",
    "        tag_list = []\n",
    "        for line in f:\n",
    "            if line != '\\n':\n",
    "                word, tag = line.strip('\\n').split()\n",
    "                word_list.append(word)\n",
    "                tag_list.append(tag)\n",
    "            else:\n",
    "                word_lists.append(word_list)\n",
    "                tag_lists.append(tag_list)\n",
    "                word_list = []\n",
    "                tag_list = []\n",
    "\n",
    "    # 如果make_vocab为True，还需要返回word2id和tag2id\n",
    "    if make_vocab:\n",
    "        word2id = build_map(word_lists)\n",
    "        tag2id = build_map(tag_lists)\n",
    "        return word_lists, tag_lists, word2id, tag2id\n",
    "    else:\n",
    "        return word_lists, tag_lists\n",
    "\n",
    "\n",
    "def build_map(lists):\n",
    "    maps = {}\n",
    "    for list_ in lists:\n",
    "        for e in list_:\n",
    "            if e not in maps:\n",
    "                maps[e] = len(maps)\n",
    "\n",
    "    return maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_lists, train_tag_lists, word2id, tag2id = build_corpus(\"train\")\n",
    "dev_word_lists, dev_tag_lists = build_corpus(\"dev\", make_vocab=False)\n",
    "test_word_lists, test_tag_lists = build_corpus(\"test\", make_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集输入数据： 高勇：男，中国国籍，无境外居留权，\n",
      "训练集标签数据： B-NAME E-NAME O O O B-CONT M-CONT M-CONT E-CONT O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "print(\"训练集输入数据：\", ''.join(train_word_lists[0]))\n",
    "print(\"训练集标签数据：\", ' '.join(train_tag_lists[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_name):\n",
    "    \"\"\"用于保存模型\"\"\"\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n",
    "def load_model(file_name):\n",
    "    \"\"\"用于加载模型\"\"\"\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "def flatten_lists(lists):\n",
    "    flatten_list = []\n",
    "    for l in lists:\n",
    "        if type(l) == list:\n",
    "            flatten_list += l\n",
    "        else:\n",
    "            flatten_list.append(l)\n",
    "    return flatten_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM(object):\n",
    "    def __init__(self, N, M):\n",
    "        \"\"\"Args:\n",
    "            N: 状态数，这里对应存在的标注的种类\n",
    "            M: 观测数，这里对应有多少不同的字\n",
    "        \"\"\"\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "\n",
    "        # 状态转移概率矩阵 A[i][j]表示从i状态转移到j状态的概率\n",
    "        self.A = torch.zeros(N, N)\n",
    "        # 观测概率矩阵, B[i][j]表示i状态下生成j观测的概率\n",
    "        self.B = torch.zeros(N, M)\n",
    "        # 初始状态概率  Pi[i]表示初始时刻为状态i的概率\n",
    "        self.Pi = torch.zeros(N)\n",
    "\n",
    "    def train(self, word_lists, tag_lists, word2id, tag2id):\n",
    "        \"\"\"HMM的训练，即根据训练语料对模型参数进行估计,\n",
    "           因为我们有观测序列以及其对应的状态序列，所以我们\n",
    "           可以使用极大似然估计的方法来估计隐马尔可夫模型的参数\n",
    "        参数:\n",
    "            word_lists: 列表，其中每个元素由字组成的列表，如 ['担','任','科','员']\n",
    "            tag_lists: 列表，其中每个元素是由对应的标注组成的列表，如 ['O','O','B-TITLE', 'E-TITLE']\n",
    "            word2id: 将字映射为ID\n",
    "            tag2id: 字典，将标注映射为ID\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(tag_lists) == len(word_lists)\n",
    "\n",
    "        # 状态转移概率矩阵：将文本中出现的状态转移现象记录，然后求其概率分布\n",
    "        for tag_list in tag_lists:\n",
    "            seq_len = len(tag_list)\n",
    "            for i in range(seq_len - 1):\n",
    "                current_tagid = tag2id[tag_list[i]]\n",
    "                next_tagid = tag2id[tag_list[i+1]]\n",
    "                self.A[current_tagid][next_tagid] += 1\n",
    "        # 问题：如果某元素没有出现过，该位置为0，这在后续的计算中是不允许的\n",
    "        # 解决方法：我们将等于0的概率加上很小的数\n",
    "        self.A[self.A == 0.] = 1e-10\n",
    "        self.A = self.A / self.A.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # 观测概率矩阵：将文本中出现的状态发射到观测值的现象记录，然后求其概率分布\n",
    "        for tag_list, word_list in zip(tag_lists, word_lists):\n",
    "            assert len(tag_list) == len(word_list)\n",
    "            for tag, word in zip(tag_list, word_list):\n",
    "                tag_id = tag2id[tag]\n",
    "                word_id = word2id[word]\n",
    "                self.B[tag_id][word_id] += 1\n",
    "        self.B[self.B == 0.] = 1e-10\n",
    "        self.B = self.B / self.B.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # 估计初始状态概率\n",
    "        for tag_list in tag_lists:\n",
    "            init_tagid = tag2id[tag_list[0]]\n",
    "            self.Pi[init_tagid] += 1\n",
    "        self.Pi[self.Pi == 0.] = 1e-10\n",
    "        self.Pi = self.Pi / self.Pi.sum()\n",
    "\n",
    "    def test(self, word_lists, word2id, tag2id):\n",
    "        pred_tag_lists = []\n",
    "        for word_list in word_lists:\n",
    "            pred_tag_list = self.decoding(word_list, word2id, tag2id)\n",
    "            pred_tag_lists.append(pred_tag_list)\n",
    "        return pred_tag_lists\n",
    "\n",
    "    def decoding(self, word_list, word2id, tag2id):\n",
    "        \"\"\"\n",
    "        使用维特比算法对给定观测序列求状态序列， 这里就是对字组成的序列,求其对应的标注。\n",
    "        维特比算法实际是用动态规划解隐马尔可夫模型预测问题，即用动态规划求概率最大路径（最优路径）\n",
    "        这时一条路径对应着一个状态序列\n",
    "        \"\"\"\n",
    "        # 问题:整条链很长的情况下，十分多的小概率相乘，最后可能造成下溢\n",
    "        # 解决办法：采用对数概率，这样源空间中的很小概率，就被映射到对数空间的大的负数\n",
    "        #  同时相乘操作也变成简单的相加操作\n",
    "        A = torch.log(self.A)\n",
    "        B = torch.log(self.B)\n",
    "        Pi = torch.log(self.Pi)\n",
    "\n",
    "        # 初始化 维比特矩阵viterbi 它的维度为[状态数, 序列长度]\n",
    "        # 其中viterbi[i, j]表示标注序列的第j个标注为i的所有单个序列(i_1, i_2, ..i_j)出现的概率最大值\n",
    "        seq_len = len(word_list)\n",
    "        viterbi = torch.zeros(self.N, seq_len)\n",
    "        # backpointer是跟viterbi一样大小的矩阵\n",
    "        # backpointer[i, j]存储的是 标注序列的第j个标注为i时，第j-1个标注的id\n",
    "        # 等解码的时候，我们用backpointer进行回溯，以求出最优路径\n",
    "        backpointer = torch.zeros(self.N, seq_len).long()\n",
    "\n",
    "        # self.Pi[i] 表示第一个字的标记为i的概率\n",
    "        # Bt[word_id]表示字为word_id的时候，对应各个标记的概率\n",
    "        # self.A.t()[tag_id]表示各个状态转移到tag_id对应的概率\n",
    "\n",
    "        # 所以第一步为\n",
    "        start_wordid = word2id.get(word_list[0], None)\n",
    "        Bt = B.t()\n",
    "        if start_wordid is None:\n",
    "            # 如果字不再字典里，则假设状态的概率分布是均匀的\n",
    "            bt = torch.log(torch.ones(self.N) / self.N)\n",
    "        else:\n",
    "            bt = Bt[start_wordid]\n",
    "        viterbi[:, 0] = Pi + bt\n",
    "        backpointer[:, 0] = -1\n",
    "\n",
    "        # 递推公式：\n",
    "        # viterbi[tag_id, step] = max(viterbi[:, step-1]* self.A.t()[tag_id] * Bt[word])\n",
    "        # 其中word是step时刻对应的字\n",
    "        # 由上述递推公式求后续各步\n",
    "        for step in range(1, seq_len):\n",
    "            wordid = word2id.get(word_list[step], None)\n",
    "            # 处理字不在字典中的情况\n",
    "            # bt是在t时刻字为wordid时，状态的概率分布\n",
    "            if wordid is None:\n",
    "                # 如果字不再字典里，则假设状态的概率分布是均匀的\n",
    "                bt = torch.log(torch.ones(self.N) / self.N)\n",
    "            else:\n",
    "                bt = Bt[wordid]  # 否则从观测概率矩阵中取bt\n",
    "            for tag_id in range(len(tag2id)):\n",
    "                max_prob, max_id = torch.max(\n",
    "                    viterbi[:, step-1] + A[:, tag_id],\n",
    "                    dim=0\n",
    "                )\n",
    "                viterbi[tag_id, step] = max_prob + bt[tag_id]\n",
    "                backpointer[tag_id, step] = max_id\n",
    "\n",
    "        # 终止， t=seq_len 即 viterbi[:, seq_len]中的最大概率，就是最优路径的概率\n",
    "        best_path_prob, best_path_pointer = torch.max(\n",
    "            viterbi[:, seq_len-1], dim=0\n",
    "        )\n",
    "\n",
    "        # 回溯，求最优路径\n",
    "        best_path_pointer = best_path_pointer.item()\n",
    "        best_path = [best_path_pointer]\n",
    "        for back_step in range(seq_len-1, 0, -1):\n",
    "            best_path_pointer = backpointer[best_path_pointer, back_step]\n",
    "            best_path_pointer = best_path_pointer.item()\n",
    "            best_path.append(best_path_pointer)\n",
    "\n",
    "        # 将tag_id组成的序列转化为tag\n",
    "        assert len(best_path) == len(word_list)\n",
    "        id2tag = dict((id_, tag) for tag, id_ in tag2id.items())\n",
    "        tag_list = [id2tag[id_] for id_ in reversed(best_path)]\n",
    "\n",
    "        return tag_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(object):\n",
    "    \"\"\"用于评价模型，计算每个标签的精确率，召回率，F1分数\"\"\"\n",
    "\n",
    "    def __init__(self, golden_tags, predict_tags, remove_O=False):\n",
    "\n",
    "        # [[t1, t2], [t3, t4]...] --> [t1, t2, t3, t4...]\n",
    "        self.golden_tags = flatten_lists(golden_tags)\n",
    "        self.predict_tags = flatten_lists(predict_tags)\n",
    "\n",
    "        if remove_O:  # 将O标记移除，只关心实体标记\n",
    "            self._remove_Otags()\n",
    "\n",
    "        # 辅助计算的变量\n",
    "        self.tagset = set(self.golden_tags)\n",
    "        self.correct_tags_number = self.count_correct_tags()\n",
    "        self.predict_tags_counter = Counter(self.predict_tags)\n",
    "        self.golden_tags_counter = Counter(self.golden_tags)\n",
    "\n",
    "        # 计算精确率\n",
    "        self.precision_scores = self.cal_precision()\n",
    "\n",
    "        # 计算召回率\n",
    "        self.recall_scores = self.cal_recall()\n",
    "\n",
    "        # 计算F1分数\n",
    "        self.f1_scores = self.cal_f1()\n",
    "\n",
    "    def cal_precision(self):\n",
    "\n",
    "        precision_scores = {}\n",
    "        for tag in self.tagset:\n",
    "            precision_scores[tag] = self.correct_tags_number.get(tag, 0) / \\\n",
    "                self.predict_tags_counter[tag]\n",
    "\n",
    "        return precision_scores\n",
    "\n",
    "    def cal_recall(self):\n",
    "\n",
    "        recall_scores = {}\n",
    "        for tag in self.tagset:\n",
    "            recall_scores[tag] = self.correct_tags_number.get(tag, 0) / \\\n",
    "                self.golden_tags_counter[tag]\n",
    "        return recall_scores\n",
    "\n",
    "    def cal_f1(self):\n",
    "        f1_scores = {}\n",
    "        for tag in self.tagset:\n",
    "            p, r = self.precision_scores[tag], self.recall_scores[tag]\n",
    "            f1_scores[tag] = 2*p*r / (p+r+1e-10)  # 加上一个特别小的数，防止分母为0\n",
    "        return f1_scores\n",
    "\n",
    "    def report_scores(self):\n",
    "        \"\"\"将结果用表格的形式打印出来，像这个样子：\n",
    "\n",
    "                      precision    recall  f1-score   support\n",
    "              B-LOC      0.775     0.757     0.766      1084\n",
    "              I-LOC      0.601     0.631     0.616       325\n",
    "             B-MISC      0.698     0.499     0.582       339\n",
    "             I-MISC      0.644     0.567     0.603       557\n",
    "              B-ORG      0.795     0.801     0.798      1400\n",
    "              I-ORG      0.831     0.773     0.801      1104\n",
    "              B-PER      0.812     0.876     0.843       735\n",
    "              I-PER      0.873     0.931     0.901       634\n",
    "\n",
    "          avg/total      0.779     0.764     0.770      6178\n",
    "        \"\"\"\n",
    "        # 打印表头\n",
    "        header_format = '{:>9s}  {:>9} {:>9} {:>9} {:>9}'\n",
    "        header = ['precision', 'recall', 'f1-score', 'support']\n",
    "        print(header_format.format('', *header))\n",
    "\n",
    "        row_format = '{:>9s}  {:>9.4f} {:>9.4f} {:>9.4f} {:>9}'\n",
    "        # 打印每个标签的 精确率、召回率、f1分数\n",
    "        for tag in self.tagset:\n",
    "            print(row_format.format(\n",
    "                tag,\n",
    "                self.precision_scores[tag],\n",
    "                self.recall_scores[tag],\n",
    "                self.f1_scores[tag],\n",
    "                self.golden_tags_counter[tag]\n",
    "            ))\n",
    "\n",
    "        # 计算并打印平均值\n",
    "        avg_metrics = self._cal_weighted_average()\n",
    "        print(row_format.format(\n",
    "            'avg/total',\n",
    "            avg_metrics['precision'],\n",
    "            avg_metrics['recall'],\n",
    "            avg_metrics['f1_score'],\n",
    "            len(self.golden_tags)\n",
    "        ))\n",
    "\n",
    "    def count_correct_tags(self):\n",
    "        \"\"\"计算每种标签预测正确的个数(对应精确率、召回率计算公式上的tp)，用于后面精确率以及召回率的计算\"\"\"\n",
    "        correct_dict = {}\n",
    "        for gold_tag, predict_tag in zip(self.golden_tags, self.predict_tags):\n",
    "            if gold_tag == predict_tag:\n",
    "                if gold_tag not in correct_dict:\n",
    "                    correct_dict[gold_tag] = 1\n",
    "                else:\n",
    "                    correct_dict[gold_tag] += 1\n",
    "\n",
    "        return correct_dict\n",
    "\n",
    "    def _cal_weighted_average(self):\n",
    "\n",
    "        weighted_average = {}\n",
    "        total = len(self.golden_tags)\n",
    "\n",
    "        # 计算weighted precisions:\n",
    "        weighted_average['precision'] = 0.\n",
    "        weighted_average['recall'] = 0.\n",
    "        weighted_average['f1_score'] = 0.\n",
    "        for tag in self.tagset:\n",
    "            size = self.golden_tags_counter[tag]\n",
    "            weighted_average['precision'] += self.precision_scores[tag] * size\n",
    "            weighted_average['recall'] += self.recall_scores[tag] * size\n",
    "            weighted_average['f1_score'] += self.f1_scores[tag] * size\n",
    "\n",
    "        for metric in weighted_average.keys():\n",
    "            weighted_average[metric] /= total\n",
    "\n",
    "        return weighted_average\n",
    "\n",
    "    def _remove_Otags(self):\n",
    "\n",
    "        length = len(self.golden_tags)\n",
    "        O_tag_indices = [i for i in range(length)\n",
    "                         if self.golden_tags[i] == 'O']\n",
    "\n",
    "        self.golden_tags = [tag for i, tag in enumerate(self.golden_tags)\n",
    "                            if i not in O_tag_indices]\n",
    "\n",
    "        self.predict_tags = [tag for i, tag in enumerate(self.predict_tags)\n",
    "                             if i not in O_tag_indices]\n",
    "        print(\"原总标记数为{}，移除了{}个O标记，占比{:.2f}%\".format(\n",
    "            length,\n",
    "            len(O_tag_indices),\n",
    "            len(O_tag_indices) / length * 100\n",
    "        ))\n",
    "\n",
    "    def report_confusion_matrix(self):\n",
    "        \"\"\"计算混淆矩阵\"\"\"\n",
    "\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        tag_list = list(self.tagset)\n",
    "        # 初始化混淆矩阵 matrix[i][j]表示第i个tag被模型预测成第j个tag的次数\n",
    "        tags_size = len(tag_list)\n",
    "        matrix = []\n",
    "        for i in range(tags_size):\n",
    "            matrix.append([0] * tags_size)\n",
    "\n",
    "        # 遍历tags列表\n",
    "        for golden_tag, predict_tag in zip(self.golden_tags, self.predict_tags):\n",
    "            try:\n",
    "                row = tag_list.index(golden_tag)\n",
    "                col = tag_list.index(predict_tag)\n",
    "                matrix[row][col] += 1\n",
    "            except ValueError:  # 有极少数标记没有出现在golden_tags，但出现在predict_tags，跳过这些标记\n",
    "                continue\n",
    "\n",
    "        # 输出矩阵\n",
    "        row_format_ = '{:>7} ' * (tags_size+1)\n",
    "        print(row_format_.format(\"\", *tag_list))\n",
    "        for i, row in enumerate(matrix):\n",
    "            print(row_format_.format(tag_list[i], *row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmm_train_eval(train_data, test_data, word2id, tag2id, remove_O=False):\n",
    "    \"\"\"训练并评估hmm模型\"\"\"\n",
    "    # 训练HMM模型\n",
    "    train_word_lists, train_tag_lists = train_data\n",
    "    test_word_lists, test_tag_lists = test_data\n",
    "\n",
    "    hmm_model = HMM(len(tag2id), len(word2id))\n",
    "    hmm_model.train(train_word_lists,\n",
    "                    train_tag_lists,\n",
    "                    word2id,\n",
    "                    tag2id)\n",
    "    save_model(hmm_model, \"./ckpts/hmm.pkl\")\n",
    "\n",
    "    # 评估hmm模型\n",
    "    pred_tag_lists = hmm_model.test(test_word_lists,\n",
    "                                    word2id,\n",
    "                                    tag2id)\n",
    "\n",
    "    metrics = Metrics(test_tag_lists, pred_tag_lists, remove_O=remove_O)\n",
    "    metrics.report_scores()\n",
    "    metrics.report_confusion_matrix()\n",
    "\n",
    "    return pred_tag_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在训练评估HMM模型...\n",
      "           precision    recall  f1-score   support\n",
      "    E-PRO     0.6512    0.8485    0.7368        33\n",
      "    E-EDU     0.9167    0.9821    0.9483       112\n",
      "    B-EDU     0.9000    0.9643    0.9310       112\n",
      "    M-EDU     0.9348    0.9609    0.9477       179\n",
      "   E-RACE     1.0000    0.9286    0.9630        14\n",
      "   E-CONT     0.9655    1.0000    0.9825        28\n",
      "  E-TITLE     0.9514    0.9637    0.9575       772\n",
      "    B-PRO     0.5581    0.7273    0.6316        33\n",
      "   B-CONT     0.9655    1.0000    0.9825        28\n",
      "    E-LOC     0.5000    0.5000    0.5000         6\n",
      "    B-LOC     0.3333    0.3333    0.3333         6\n",
      "    M-PRO     0.4490    0.6471    0.5301        68\n",
      "   B-RACE     1.0000    0.9286    0.9630        14\n",
      "   B-NAME     0.9800    0.8750    0.9245       112\n",
      "    M-LOC     0.5833    0.3333    0.4242        21\n",
      "    B-ORG     0.8422    0.8879    0.8644       553\n",
      "  B-TITLE     0.8811    0.8925    0.8867       772\n",
      "   M-NAME     0.9459    0.8537    0.8974        82\n",
      "        O     0.9568    0.9177    0.9369      5190\n",
      "   M-CONT     0.9815    1.0000    0.9907        53\n",
      "    E-ORG     0.8262    0.8680    0.8466       553\n",
      "  M-TITLE     0.9038    0.8751    0.8892      1922\n",
      "   E-NAME     0.9000    0.8036    0.8491       112\n",
      "    M-ORG     0.9002    0.9327    0.9162      4325\n",
      "avg/total     0.9149    0.9122    0.9130     15100\n",
      "\n",
      "Confusion Matrix:\n",
      "          E-PRO   E-EDU   B-EDU   M-EDU  E-RACE  E-CONT E-TITLE   B-PRO  B-CONT   E-LOC   B-LOC   M-PRO  B-RACE  B-NAME   M-LOC   B-ORG B-TITLE  M-NAME       O  M-CONT   E-ORG M-TITLE  E-NAME   M-ORG \n",
      "  E-PRO      28       1       1       1       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       2       0       0       0 \n",
      "  E-EDU       1     110       0       1       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n",
      "  B-EDU       0       0     108       3       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       1 \n",
      "  M-EDU       0       0       0     172       0       0       0       1       0       0       0       4       0       0       0       0       0       0       1       0       1       0       0       0 \n",
      " E-RACE       0       0       0       0      13       0       0       0       0       0       0       0       0       0       0       0       0       0       1       0       0       0       0       0 \n",
      " E-CONT       0       0       0       0       0      28       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n",
      "E-TITLE       0       1       0       0       0       0     744       0       0       0       0       0       0       0       0       4       0       0       6       0       0       2       0      15 \n",
      "  B-PRO       0       0       1       0       0       0       0      24       0       0       0       3       0       0       0       0       0       0       0       0       0       0       0       5 \n",
      " B-CONT       0       0       0       0       0       0       0       0      28       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n",
      "  E-LOC       0       0       0       0       0       0       0       0       0       3       0       0       0       0       0       0       0       0       2       0       0       0       0       1 \n",
      "  B-LOC       0       0       0       0       0       0       0       0       0       0       2       0       0       0       0       3       0       0       1       0       0       0       0       0 \n",
      "  M-PRO       0       0       1       1       0       0       0       1       0       0       0      44       0       0       0       0       0       0       0       0       3       0       0      18 \n",
      " B-RACE       0       0       0       0       0       0       0       0       0       0       0       0      13       0       0       0       0       0       1       0       0       0       0       0 \n",
      " B-NAME       0       0       0       0       0       0       0       0       0       0       0       0       0      98       0       1       0       0       8       0       0       0       0       2 \n",
      "  M-LOC       0       0       0       0       0       0       0       0       0       0       1       0       0       0       7       0       0       0       4       0       2       0       0       7 \n",
      "  B-ORG       0       0       0       0       0       0       0       0       1       0       3       0       0       1       0     491       6       0      28       0       0       0       0      23 \n",
      "B-TITLE       0       0       2       0       0       0       1       2       0       0       0       0       0       0       0       6     689       0      20       0       1      28       0      23 \n",
      " M-NAME       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      70       3       0       0       0       6       3 \n",
      "      O       4       2       1       1       0       0      26       3       0       0       0      12       0       0       0      37      26       0    4763       0      30      78       2     204 \n",
      " M-CONT       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      53       0       0       0       0 \n",
      "  E-ORG       0       0       1       0       0       0       1       1       0       0       0       3       0       0       0       0       9       0      10       0     480      18       0      30 \n",
      "M-TITLE       3       3       2       4       0       0       6       1       0       0       0       7       0       0       0       3      35       0      44       0      17    1682       0     115 \n",
      " E-NAME       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       2      16       0       3       0      90       0 \n",
      "  M-ORG       7       3       3       1       0       1       4      10       0       3       0      25       0       1       5      38      17       2      70       1      42      53       2    4034 \n"
     ]
    }
   ],
   "source": [
    "# 训练评估ｈｍｍ模型\n",
    "print(\"正在训练评估HMM模型...\")\n",
    "hmm_pred = hmm_train_eval(\n",
    "    (train_word_lists, train_tag_lists),\n",
    "    (test_word_lists, test_tag_lists),\n",
    "    word2id,\n",
    "    tag2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
