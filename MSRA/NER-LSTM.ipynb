{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 LSTM 进行命名实体识别\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from os.path import join\n",
    "from codecs import open\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(split, make_vocab=True, data_dir=\"./ResumeNER\"):\n",
    "    \"\"\"读取数据\"\"\"\n",
    "    assert split in ['train', 'dev', 'test']\n",
    "\n",
    "    word_lists = []\n",
    "    tag_lists = []\n",
    "    with open(join(data_dir, split+\".char.bmes\"), 'r', encoding='utf-8') as f:\n",
    "        word_list = []\n",
    "        tag_list = []\n",
    "        for line in f:\n",
    "            if line != '\\n':\n",
    "                word, tag = line.strip('\\n').split()\n",
    "                word_list.append(word)\n",
    "                tag_list.append(tag)\n",
    "            else:\n",
    "                word_lists.append(word_list)\n",
    "                tag_lists.append(tag_list)\n",
    "                word_list = []\n",
    "                tag_list = []\n",
    "\n",
    "    # 如果make_vocab为True，还需要返回word2id和tag2id\n",
    "    if make_vocab:\n",
    "        word2id = build_map(word_lists)\n",
    "        tag2id = build_map(tag_lists)\n",
    "        return word_lists, tag_lists, word2id, tag2id\n",
    "    else:\n",
    "        return word_lists, tag_lists\n",
    "\n",
    "\n",
    "def build_map(lists):\n",
    "    maps = {}\n",
    "    for list_ in lists:\n",
    "        for e in list_:\n",
    "            if e not in maps:\n",
    "                maps[e] = len(maps)\n",
    "\n",
    "    return maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_lists, train_tag_lists, word2id, tag2id = build_corpus(\"train\")\n",
    "dev_word_lists, dev_tag_lists = build_corpus(\"dev\", make_vocab=False)\n",
    "test_word_lists, test_tag_lists = build_corpus(\"test\", make_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集输入数据： 高勇：男，中国国籍，无境外居留权，\n",
      "训练集标签数据： B-NAME E-NAME O O O B-CONT M-CONT M-CONT E-CONT O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "print(\"训练集输入数据：\", ''.join(train_word_lists[0]))\n",
    "print(\"训练集标签数据：\", ' '.join(train_tag_lists[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTM模型训练的时候需要在word2id和tag2id加入PAD和UNK\n",
    "- 如果是加了CRF的lstm还要加入<start>和<end> (解码的时候需要用到)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_maps(word2id, tag2id):\n",
    "    '''\n",
    "    添加 PAD 和 UNK 索引\n",
    "    '''\n",
    "    word2id['<unk>'] = len(word2id)\n",
    "    word2id['<pad>'] = len(word2id)\n",
    "    tag2id['<unk>'] = len(tag2id)\n",
    "    tag2id['<pad>'] = len(tag2id)\n",
    "    \n",
    "    return word2id, tag2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK 的索引：1792\n",
      "PAD 的索引：1793\n"
     ]
    }
   ],
   "source": [
    "# LSTM模型训练的时候需要在word2id和tag2id加入PAD和UNK\n",
    "bilstm_word2id, bilstm_tag2id = extend_maps(word2id, tag2id)\n",
    "print('UNK 的索引：{}'.format(bilstm_word2id.get('<unk>')))\n",
    "print('PAD 的索引：{}'.format(bilstm_word2id.get('<pad>')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, out_size):\n",
    "        \"\"\"初始化参数：\n",
    "            vocab_size:字典的大小\n",
    "            emb_size:词向量的维数\n",
    "            hidden_size：隐向量的维数\n",
    "            out_size:标注的种类\n",
    "        \"\"\"\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.bilstm = nn.LSTM(emb_size, hidden_size,\n",
    "                              batch_first=True,\n",
    "                              bidirectional=True)\n",
    "\n",
    "        self.lin = nn.Linear(2*hidden_size, out_size)\n",
    "\n",
    "    def forward(self, sents_tensor, lengths):\n",
    "        emb = self.embedding(sents_tensor)  # [B, L, emb_size]\n",
    "\n",
    "        packed = pack_padded_sequence(emb, lengths, batch_first=True)\n",
    "        rnn_out, _ = self.bilstm(packed)\n",
    "        # rnn_out:[B, L, hidden_size*2]\n",
    "        rnn_out, _ = pad_packed_sequence(rnn_out, batch_first=True)\n",
    "\n",
    "        scores = self.lin(rnn_out)  # [B, L, out_size]\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def test(self, sents_tensor, lengths):\n",
    "        \"\"\"\n",
    "        测试\n",
    "        \"\"\"\n",
    "        logits = self.forward(sents_tensor, lengths)  # [B, L, out_size]\n",
    "        _, batch_tagids = torch.max(logits, dim=2)\n",
    "\n",
    "        return batch_tagids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******** LSTM模型 工具函数*************\n",
    "\n",
    "def tensorized(batch, maps):\n",
    "    '''\n",
    "    将分割好的 batch 数据，转换为等长的tensor\n",
    "    参数\n",
    "    batch：输入的 batch 数据，列表\n",
    "    maps：字到索引的字典\n",
    "    '''\n",
    "    PAD = maps.get('<pad>')\n",
    "    UNK = maps.get('<unk>')\n",
    "\n",
    "    max_len = len(batch[0])  # 由于排序了，所以选取第一个最长字符串为 max_len\n",
    "    batch_size = len(batch)\n",
    "    # 构建 输入 tensor\n",
    "    batch_tensor = torch.ones(batch_size, max_len).long() * PAD\n",
    "    for i, l in enumerate(batch):\n",
    "        for j, e in enumerate(l):\n",
    "            batch_tensor[i][j] = maps.get(e, UNK)\n",
    "    # batch各个元素的长度\n",
    "    lengths = [len(l) for l in batch]\n",
    "\n",
    "    return batch_tensor, lengths\n",
    "\n",
    "\n",
    "def sort_by_lengths(word_lists, tag_lists):\n",
    "    '''\n",
    "    将数据排序，最长的句子在第一个，为使用函数pack_padded_sequence() 做准备\n",
    "    '''\n",
    "    pairs = list(zip(word_lists, tag_lists))\n",
    "    # 根据 序列长度，对序列的 index 排序！\n",
    "    indices = sorted(range(len(pairs)),\n",
    "                     key=lambda k: len(pairs[k][0]),\n",
    "                     reverse=True)\n",
    "    pairs = [pairs[i] for i in indices]\n",
    "    # pairs.sort(key=lambda pair: len(pair[0]), reverse=True)\n",
    "    # 添加 * 表示 解压\n",
    "    word_lists, tag_lists = list(zip(*pairs))\n",
    "\n",
    "    return word_lists, tag_lists, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss(logits, targets, tag2id):\n",
    "    \"\"\"\n",
    "    计算损失\n",
    "    参数:\n",
    "        logits: [B, L, out_size]\n",
    "        targets: [B, L]\n",
    "        lengths: [B]\n",
    "    \"\"\"\n",
    "    PAD = tag2id.get('<pad>')\n",
    "    assert PAD is not None\n",
    "    # 选取非 PAD 数据\n",
    "    mask = (targets != PAD)  # [B, L]\n",
    "    targets = targets[mask]\n",
    "    out_size = logits.size(2)\n",
    "    logits = logits.masked_select(  # 返回一维张量\n",
    "        mask.unsqueeze(2).expand(-1, -1, out_size)  # 将mask 扩展成 [B, L, out_size] \n",
    "    ).contiguous().view(-1, out_size)\n",
    "\n",
    "    assert logits.size(0) == targets.size(0)\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# 设置GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置lstm训练参数\n",
    "class TrainingConfig(object):\n",
    "    batch_size = 64\n",
    "    # 学习速率\n",
    "    lr = 0.001\n",
    "    epoches = 8\n",
    "    print_step = 5\n",
    "\n",
    "\n",
    "class LSTMConfig(object):\n",
    "    emb_size = 128  # 词向量的维数\n",
    "    hidden_size = 128  # lstm隐向量的维数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size:词典大小\n",
    "# out_size:标注种类\n",
    "vocab_size = len(word2id)\n",
    "out_size = len(tag2id)\n",
    "# 加载模型参数\n",
    "emb_size = LSTMConfig.emb_size\n",
    "hidden_size = LSTMConfig.hidden_size\n",
    "\n",
    "model = BiLSTM(vocab_size, emb_size, hidden_size, out_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练参数：\n",
    "epoches = TrainingConfig.epoches\n",
    "print_step = TrainingConfig.print_step\n",
    "lr = TrainingConfig.lr\n",
    "batch_size = TrainingConfig.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化其他指标\n",
    "step = 0\n",
    "best_val_loss = 1e18\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dev_word_lists, dev_tag_lists, word2id, tag2id):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = 0.\n",
    "        val_step = 0\n",
    "        for ind in range(0, len(dev_word_lists), batch_size):\n",
    "            val_step += 1\n",
    "            # 准备batch数据\n",
    "            batch_sents = dev_word_lists[ind:ind+batch_size]\n",
    "            batch_tags = dev_tag_lists[ind:ind+batch_size]\n",
    "            tensorized_sents, lengths = tensorized(batch_sents, word2id)\n",
    "            tensorized_sents = tensorized_sents.to(device)\n",
    "            targets, lengths = tensorized(batch_tags, tag2id)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward\n",
    "            scores = model(tensorized_sents, lengths)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = cal_loss(scores, targets, tag2id).to(device)\n",
    "            val_losses += loss.item()\n",
    "        val_loss = val_losses / val_step\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            print(\"保存模型...\")\n",
    "            best_model = model\n",
    "            _best_val_loss = val_loss\n",
    "\n",
    "        return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, step/total_step: 5/60 8.33% Loss:3.2675\n",
      "Epoch 1, step/total_step: 10/60 16.67% Loss:2.9339\n",
      "Epoch 1, step/total_step: 15/60 25.00% Loss:2.3871\n",
      "Epoch 1, step/total_step: 20/60 33.33% Loss:1.5599\n",
      "Epoch 1, step/total_step: 25/60 41.67% Loss:1.2821\n",
      "Epoch 1, step/total_step: 30/60 50.00% Loss:1.1483\n",
      "Epoch 1, step/total_step: 35/60 58.33% Loss:1.0458\n",
      "Epoch 1, step/total_step: 40/60 66.67% Loss:1.0502\n",
      "Epoch 1, step/total_step: 45/60 75.00% Loss:0.9976\n",
      "Epoch 1, step/total_step: 50/60 83.33% Loss:0.9972\n",
      "Epoch 1, step/total_step: 55/60 91.67% Loss:1.4571\n",
      "Epoch 1, step/total_step: 60/60 100.00% Loss:1.4277\n",
      "保存模型...\n",
      "Epoch 1, Val Loss:0.9873\n",
      "Epoch 2, step/total_step: 5/60 8.33% Loss:1.2247\n",
      "Epoch 2, step/total_step: 10/60 16.67% Loss:0.8507\n",
      "Epoch 2, step/total_step: 15/60 25.00% Loss:0.7380\n",
      "Epoch 2, step/total_step: 20/60 33.33% Loss:0.6014\n",
      "Epoch 2, step/total_step: 25/60 41.67% Loss:0.5827\n",
      "Epoch 2, step/total_step: 30/60 50.00% Loss:0.5087\n",
      "Epoch 2, step/total_step: 35/60 58.33% Loss:0.4742\n",
      "Epoch 2, step/total_step: 40/60 66.67% Loss:0.5017\n",
      "Epoch 2, step/total_step: 45/60 75.00% Loss:0.4728\n",
      "Epoch 2, step/total_step: 50/60 83.33% Loss:0.5020\n",
      "Epoch 2, step/total_step: 55/60 91.67% Loss:0.7374\n",
      "Epoch 2, step/total_step: 60/60 100.00% Loss:0.7224\n",
      "保存模型...\n",
      "Epoch 2, Val Loss:0.4723\n",
      "Epoch 3, step/total_step: 5/60 8.33% Loss:0.5225\n",
      "Epoch 3, step/total_step: 10/60 16.67% Loss:0.5062\n",
      "Epoch 3, step/total_step: 15/60 25.00% Loss:0.4216\n",
      "Epoch 3, step/total_step: 20/60 33.33% Loss:0.3327\n",
      "Epoch 3, step/total_step: 25/60 41.67% Loss:0.3340\n",
      "Epoch 3, step/total_step: 30/60 50.00% Loss:0.2883\n",
      "Epoch 3, step/total_step: 35/60 58.33% Loss:0.2769\n",
      "Epoch 3, step/total_step: 40/60 66.67% Loss:0.2983\n",
      "Epoch 3, step/total_step: 45/60 75.00% Loss:0.2831\n",
      "Epoch 3, step/total_step: 50/60 83.33% Loss:0.2965\n",
      "Epoch 3, step/total_step: 55/60 91.67% Loss:0.3900\n",
      "Epoch 3, step/total_step: 60/60 100.00% Loss:0.4348\n",
      "保存模型...\n",
      "Epoch 3, Val Loss:0.3267\n",
      "Epoch 4, step/total_step: 5/60 8.33% Loss:0.3536\n",
      "Epoch 4, step/total_step: 10/60 16.67% Loss:0.3380\n",
      "Epoch 4, step/total_step: 15/60 25.00% Loss:0.2784\n",
      "Epoch 4, step/total_step: 20/60 33.33% Loss:0.2118\n",
      "Epoch 4, step/total_step: 25/60 41.67% Loss:0.2284\n",
      "Epoch 4, step/total_step: 30/60 50.00% Loss:0.1960\n",
      "Epoch 4, step/total_step: 35/60 58.33% Loss:0.1949\n",
      "Epoch 4, step/total_step: 40/60 66.67% Loss:0.2113\n",
      "Epoch 4, step/total_step: 45/60 75.00% Loss:0.1971\n",
      "Epoch 4, step/total_step: 50/60 83.33% Loss:0.2008\n",
      "Epoch 4, step/total_step: 55/60 91.67% Loss:0.2518\n",
      "Epoch 4, step/total_step: 60/60 100.00% Loss:0.2915\n",
      "保存模型...\n",
      "Epoch 4, Val Loss:0.2540\n",
      "Epoch 5, step/total_step: 5/60 8.33% Loss:0.2741\n",
      "Epoch 5, step/total_step: 10/60 16.67% Loss:0.2535\n",
      "Epoch 5, step/total_step: 15/60 25.00% Loss:0.2136\n",
      "Epoch 5, step/total_step: 20/60 33.33% Loss:0.1541\n",
      "Epoch 5, step/total_step: 25/60 41.67% Loss:0.1672\n",
      "Epoch 5, step/total_step: 30/60 50.00% Loss:0.1437\n",
      "Epoch 5, step/total_step: 35/60 58.33% Loss:0.1464\n",
      "Epoch 5, step/total_step: 40/60 66.67% Loss:0.1587\n",
      "Epoch 5, step/total_step: 45/60 75.00% Loss:0.1466\n",
      "Epoch 5, step/total_step: 50/60 83.33% Loss:0.1472\n",
      "Epoch 5, step/total_step: 55/60 91.67% Loss:0.1756\n",
      "Epoch 5, step/total_step: 60/60 100.00% Loss:0.1990\n",
      "保存模型...\n",
      "Epoch 5, Val Loss:0.2149\n",
      "Epoch 6, step/total_step: 5/60 8.33% Loss:0.2310\n",
      "Epoch 6, step/total_step: 10/60 16.67% Loss:0.2041\n",
      "Epoch 6, step/total_step: 15/60 25.00% Loss:0.1738\n",
      "Epoch 6, step/total_step: 20/60 33.33% Loss:0.1192\n",
      "Epoch 6, step/total_step: 25/60 41.67% Loss:0.1300\n",
      "Epoch 6, step/total_step: 30/60 50.00% Loss:0.1105\n",
      "Epoch 6, step/total_step: 35/60 58.33% Loss:0.1141\n",
      "Epoch 6, step/total_step: 40/60 66.67% Loss:0.1222\n",
      "Epoch 6, step/total_step: 45/60 75.00% Loss:0.1130\n",
      "Epoch 6, step/total_step: 50/60 83.33% Loss:0.1107\n",
      "Epoch 6, step/total_step: 55/60 91.67% Loss:0.1246\n",
      "Epoch 6, step/total_step: 60/60 100.00% Loss:0.1372\n",
      "保存模型...\n",
      "Epoch 6, Val Loss:0.1905\n",
      "Epoch 7, step/total_step: 5/60 8.33% Loss:0.1986\n",
      "Epoch 7, step/total_step: 10/60 16.67% Loss:0.1693\n",
      "Epoch 7, step/total_step: 15/60 25.00% Loss:0.1442\n",
      "Epoch 7, step/total_step: 20/60 33.33% Loss:0.0959\n",
      "Epoch 7, step/total_step: 25/60 41.67% Loss:0.1052\n",
      "Epoch 7, step/total_step: 30/60 50.00% Loss:0.0879\n",
      "Epoch 7, step/total_step: 35/60 58.33% Loss:0.0914\n",
      "Epoch 7, step/total_step: 40/60 66.67% Loss:0.0957\n",
      "Epoch 7, step/total_step: 45/60 75.00% Loss:0.0892\n",
      "Epoch 7, step/total_step: 50/60 83.33% Loss:0.0870\n",
      "Epoch 7, step/total_step: 55/60 91.67% Loss:0.0912\n",
      "Epoch 7, step/total_step: 60/60 100.00% Loss:0.0961\n",
      "保存模型...\n",
      "Epoch 7, Val Loss:0.1743\n",
      "Epoch 8, step/total_step: 5/60 8.33% Loss:0.1736\n",
      "Epoch 8, step/total_step: 10/60 16.67% Loss:0.1421\n",
      "Epoch 8, step/total_step: 15/60 25.00% Loss:0.1209\n",
      "Epoch 8, step/total_step: 20/60 33.33% Loss:0.0790\n",
      "Epoch 8, step/total_step: 25/60 41.67% Loss:0.0863\n",
      "Epoch 8, step/total_step: 30/60 50.00% Loss:0.0718\n",
      "Epoch 8, step/total_step: 35/60 58.33% Loss:0.0733\n",
      "Epoch 8, step/total_step: 40/60 66.67% Loss:0.0759\n",
      "Epoch 8, step/total_step: 45/60 75.00% Loss:0.0707\n",
      "Epoch 8, step/total_step: 50/60 83.33% Loss:0.0693\n",
      "Epoch 8, step/total_step: 55/60 91.67% Loss:0.0688\n",
      "Epoch 8, step/total_step: 60/60 100.00% Loss:0.0683\n",
      "保存模型...\n",
      "Epoch 8, Val Loss:0.1646\n",
      "Epoch 9, step/total_step: 5/60 8.33% Loss:0.1533\n",
      "Epoch 9, step/total_step: 10/60 16.67% Loss:0.1201\n",
      "Epoch 9, step/total_step: 15/60 25.00% Loss:0.1022\n",
      "Epoch 9, step/total_step: 20/60 33.33% Loss:0.0650\n",
      "Epoch 9, step/total_step: 25/60 41.67% Loss:0.0725\n",
      "Epoch 9, step/total_step: 30/60 50.00% Loss:0.0591\n",
      "Epoch 9, step/total_step: 35/60 58.33% Loss:0.0611\n",
      "Epoch 9, step/total_step: 40/60 66.67% Loss:0.0620\n",
      "Epoch 9, step/total_step: 45/60 75.00% Loss:0.0576\n",
      "Epoch 9, step/total_step: 50/60 83.33% Loss:0.0557\n",
      "Epoch 9, step/total_step: 55/60 91.67% Loss:0.0537\n",
      "Epoch 9, step/total_step: 60/60 100.00% Loss:0.0489\n",
      "保存模型...\n",
      "Epoch 9, Val Loss:0.1596\n",
      "Epoch 10, step/total_step: 5/60 8.33% Loss:0.1362\n",
      "Epoch 10, step/total_step: 10/60 16.67% Loss:0.1013\n",
      "Epoch 10, step/total_step: 15/60 25.00% Loss:0.0882\n",
      "Epoch 10, step/total_step: 20/60 33.33% Loss:0.0546\n",
      "Epoch 10, step/total_step: 25/60 41.67% Loss:0.0623\n",
      "Epoch 10, step/total_step: 30/60 50.00% Loss:0.0506\n",
      "Epoch 10, step/total_step: 35/60 58.33% Loss:0.0506\n",
      "Epoch 10, step/total_step: 40/60 66.67% Loss:0.0513\n",
      "Epoch 10, step/total_step: 45/60 75.00% Loss:0.0452\n",
      "Epoch 10, step/total_step: 50/60 83.33% Loss:0.0451\n",
      "Epoch 10, step/total_step: 55/60 91.67% Loss:0.0424\n",
      "Epoch 10, step/total_step: 60/60 100.00% Loss:0.0357\n",
      "保存模型...\n",
      "Epoch 10, Val Loss:0.1524\n",
      "Epoch 11, step/total_step: 5/60 8.33% Loss:0.1188\n",
      "Epoch 11, step/total_step: 10/60 16.67% Loss:0.0848\n",
      "Epoch 11, step/total_step: 15/60 25.00% Loss:0.0749\n",
      "Epoch 11, step/total_step: 20/60 33.33% Loss:0.0449\n",
      "Epoch 11, step/total_step: 25/60 41.67% Loss:0.0534\n",
      "Epoch 11, step/total_step: 30/60 50.00% Loss:0.0422\n",
      "Epoch 11, step/total_step: 35/60 58.33% Loss:0.0426\n",
      "Epoch 11, step/total_step: 40/60 66.67% Loss:0.0429\n",
      "Epoch 11, step/total_step: 45/60 75.00% Loss:0.0363\n",
      "Epoch 11, step/total_step: 50/60 83.33% Loss:0.0370\n",
      "Epoch 11, step/total_step: 55/60 91.67% Loss:0.0342\n",
      "Epoch 11, step/total_step: 60/60 100.00% Loss:0.0270\n",
      "保存模型...\n",
      "Epoch 11, Val Loss:0.1457\n",
      "Epoch 12, step/total_step: 5/60 8.33% Loss:0.1029\n",
      "Epoch 12, step/total_step: 10/60 16.67% Loss:0.0723\n",
      "Epoch 12, step/total_step: 15/60 25.00% Loss:0.0649\n",
      "Epoch 12, step/total_step: 20/60 33.33% Loss:0.0380\n",
      "Epoch 12, step/total_step: 25/60 41.67% Loss:0.0451\n",
      "Epoch 12, step/total_step: 30/60 50.00% Loss:0.0353\n",
      "Epoch 12, step/total_step: 35/60 58.33% Loss:0.0360\n",
      "Epoch 12, step/total_step: 40/60 66.67% Loss:0.0358\n",
      "Epoch 12, step/total_step: 45/60 75.00% Loss:0.0296\n",
      "Epoch 12, step/total_step: 50/60 83.33% Loss:0.0304\n",
      "Epoch 12, step/total_step: 55/60 91.67% Loss:0.0277\n",
      "Epoch 12, step/total_step: 60/60 100.00% Loss:0.0211\n",
      "保存模型...\n",
      "Epoch 12, Val Loss:0.1480\n",
      "Epoch 13, step/total_step: 5/60 8.33% Loss:0.0904\n",
      "Epoch 13, step/total_step: 10/60 16.67% Loss:0.0601\n",
      "Epoch 13, step/total_step: 15/60 25.00% Loss:0.0555\n",
      "Epoch 13, step/total_step: 20/60 33.33% Loss:0.0325\n",
      "Epoch 13, step/total_step: 25/60 41.67% Loss:0.0407\n",
      "Epoch 13, step/total_step: 30/60 50.00% Loss:0.0302\n",
      "Epoch 13, step/total_step: 35/60 58.33% Loss:0.0306\n",
      "Epoch 13, step/total_step: 40/60 66.67% Loss:0.0299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, step/total_step: 45/60 75.00% Loss:0.0243\n",
      "Epoch 13, step/total_step: 50/60 83.33% Loss:0.0257\n",
      "Epoch 13, step/total_step: 55/60 91.67% Loss:0.0228\n",
      "Epoch 13, step/total_step: 60/60 100.00% Loss:0.0169\n",
      "保存模型...\n",
      "Epoch 13, Val Loss:0.1459\n",
      "Epoch 14, step/total_step: 5/60 8.33% Loss:0.0784\n",
      "Epoch 14, step/total_step: 10/60 16.67% Loss:0.0550\n",
      "Epoch 14, step/total_step: 15/60 25.00% Loss:0.0514\n",
      "Epoch 14, step/total_step: 20/60 33.33% Loss:0.0293\n",
      "Epoch 14, step/total_step: 25/60 41.67% Loss:0.0332\n",
      "Epoch 14, step/total_step: 30/60 50.00% Loss:0.0244\n",
      "Epoch 14, step/total_step: 35/60 58.33% Loss:0.0266\n",
      "Epoch 14, step/total_step: 40/60 66.67% Loss:0.0255\n",
      "Epoch 14, step/total_step: 45/60 75.00% Loss:0.0200\n",
      "Epoch 14, step/total_step: 50/60 83.33% Loss:0.0214\n",
      "Epoch 14, step/total_step: 55/60 91.67% Loss:0.0188\n",
      "Epoch 14, step/total_step: 60/60 100.00% Loss:0.0135\n",
      "保存模型...\n",
      "Epoch 14, Val Loss:0.1490\n",
      "Epoch 15, step/total_step: 5/60 8.33% Loss:0.0702\n",
      "Epoch 15, step/total_step: 10/60 16.67% Loss:0.0471\n",
      "Epoch 15, step/total_step: 15/60 25.00% Loss:0.0460\n",
      "Epoch 15, step/total_step: 20/60 33.33% Loss:0.0276\n",
      "Epoch 15, step/total_step: 25/60 41.67% Loss:0.0292\n",
      "Epoch 15, step/total_step: 30/60 50.00% Loss:0.0233\n",
      "Epoch 15, step/total_step: 35/60 58.33% Loss:0.0239\n",
      "Epoch 15, step/total_step: 40/60 66.67% Loss:0.0221\n",
      "Epoch 15, step/total_step: 45/60 75.00% Loss:0.0176\n",
      "Epoch 15, step/total_step: 50/60 83.33% Loss:0.0184\n",
      "Epoch 15, step/total_step: 55/60 91.67% Loss:0.0156\n",
      "Epoch 15, step/total_step: 60/60 100.00% Loss:0.0113\n",
      "保存模型...\n",
      "Epoch 15, Val Loss:0.1458\n",
      "Epoch 16, step/total_step: 5/60 8.33% Loss:0.0639\n",
      "Epoch 16, step/total_step: 10/60 16.67% Loss:0.0434\n",
      "Epoch 16, step/total_step: 15/60 25.00% Loss:0.0437\n",
      "Epoch 16, step/total_step: 20/60 33.33% Loss:0.0236\n",
      "Epoch 16, step/total_step: 25/60 41.67% Loss:0.0278\n",
      "Epoch 16, step/total_step: 30/60 50.00% Loss:0.0221\n",
      "Epoch 16, step/total_step: 35/60 58.33% Loss:0.0241\n",
      "Epoch 16, step/total_step: 40/60 66.67% Loss:0.0190\n",
      "Epoch 16, step/total_step: 45/60 75.00% Loss:0.0174\n",
      "Epoch 16, step/total_step: 50/60 83.33% Loss:0.0172\n",
      "Epoch 16, step/total_step: 55/60 91.67% Loss:0.0132\n",
      "Epoch 16, step/total_step: 60/60 100.00% Loss:0.0096\n",
      "保存模型...\n",
      "Epoch 16, Val Loss:0.1429\n",
      "Epoch 17, step/total_step: 5/60 8.33% Loss:0.0583\n",
      "Epoch 17, step/total_step: 10/60 16.67% Loss:0.0428\n",
      "Epoch 17, step/total_step: 15/60 25.00% Loss:0.0400\n",
      "Epoch 17, step/total_step: 20/60 33.33% Loss:0.0238\n",
      "Epoch 17, step/total_step: 25/60 41.67% Loss:0.0248\n",
      "Epoch 17, step/total_step: 30/60 50.00% Loss:0.0239\n",
      "Epoch 17, step/total_step: 35/60 58.33% Loss:0.0223\n",
      "Epoch 17, step/total_step: 40/60 66.67% Loss:0.0217\n",
      "Epoch 17, step/total_step: 45/60 75.00% Loss:0.0160\n",
      "Epoch 17, step/total_step: 50/60 83.33% Loss:0.0161\n",
      "Epoch 17, step/total_step: 55/60 91.67% Loss:0.0120\n",
      "Epoch 17, step/total_step: 60/60 100.00% Loss:0.0082\n",
      "保存模型...\n",
      "Epoch 17, Val Loss:0.1457\n",
      "Epoch 18, step/total_step: 5/60 8.33% Loss:0.0629\n",
      "Epoch 18, step/total_step: 10/60 16.67% Loss:0.0356\n",
      "Epoch 18, step/total_step: 15/60 25.00% Loss:0.0404\n",
      "Epoch 18, step/total_step: 20/60 33.33% Loss:0.0237\n",
      "Epoch 18, step/total_step: 25/60 41.67% Loss:0.0275\n",
      "Epoch 18, step/total_step: 30/60 50.00% Loss:0.0242\n",
      "Epoch 18, step/total_step: 35/60 58.33% Loss:0.0294\n",
      "Epoch 18, step/total_step: 40/60 66.67% Loss:0.0173\n",
      "Epoch 18, step/total_step: 45/60 75.00% Loss:0.0194\n",
      "Epoch 18, step/total_step: 50/60 83.33% Loss:0.0141\n",
      "Epoch 18, step/total_step: 55/60 91.67% Loss:0.0109\n",
      "Epoch 18, step/total_step: 60/60 100.00% Loss:0.0072\n",
      "保存模型...\n",
      "Epoch 18, Val Loss:0.1556\n",
      "Epoch 19, step/total_step: 5/60 8.33% Loss:0.0601\n",
      "Epoch 19, step/total_step: 10/60 16.67% Loss:0.0344\n",
      "Epoch 19, step/total_step: 15/60 25.00% Loss:0.0328\n",
      "Epoch 19, step/total_step: 20/60 33.33% Loss:0.0157\n",
      "Epoch 19, step/total_step: 25/60 41.67% Loss:0.0180\n",
      "Epoch 19, step/total_step: 30/60 50.00% Loss:0.0145\n",
      "Epoch 19, step/total_step: 35/60 58.33% Loss:0.0142\n",
      "Epoch 19, step/total_step: 40/60 66.67% Loss:0.0157\n",
      "Epoch 19, step/total_step: 45/60 75.00% Loss:0.0102\n",
      "Epoch 19, step/total_step: 50/60 83.33% Loss:0.0115\n",
      "Epoch 19, step/total_step: 55/60 91.67% Loss:0.0088\n",
      "Epoch 19, step/total_step: 60/60 100.00% Loss:0.0061\n",
      "保存模型...\n",
      "Epoch 19, Val Loss:0.1438\n",
      "Epoch 20, step/total_step: 5/60 8.33% Loss:0.0481\n",
      "Epoch 20, step/total_step: 10/60 16.67% Loss:0.0306\n",
      "Epoch 20, step/total_step: 15/60 25.00% Loss:0.0272\n",
      "Epoch 20, step/total_step: 20/60 33.33% Loss:0.0124\n",
      "Epoch 20, step/total_step: 25/60 41.67% Loss:0.0134\n",
      "Epoch 20, step/total_step: 30/60 50.00% Loss:0.0105\n",
      "Epoch 20, step/total_step: 35/60 58.33% Loss:0.0127\n",
      "Epoch 20, step/total_step: 40/60 66.67% Loss:0.0106\n",
      "Epoch 20, step/total_step: 45/60 75.00% Loss:0.0082\n",
      "Epoch 20, step/total_step: 50/60 83.33% Loss:0.0097\n",
      "Epoch 20, step/total_step: 55/60 91.67% Loss:0.0075\n",
      "Epoch 20, step/total_step: 60/60 100.00% Loss:0.0053\n",
      "保存模型...\n",
      "Epoch 20, Val Loss:0.1412\n",
      "Epoch 21, step/total_step: 5/60 8.33% Loss:0.0370\n",
      "Epoch 21, step/total_step: 10/60 16.67% Loss:0.0235\n",
      "Epoch 21, step/total_step: 15/60 25.00% Loss:0.0235\n",
      "Epoch 21, step/total_step: 20/60 33.33% Loss:0.0117\n",
      "Epoch 21, step/total_step: 25/60 41.67% Loss:0.0122\n",
      "Epoch 21, step/total_step: 30/60 50.00% Loss:0.0086\n",
      "Epoch 21, step/total_step: 35/60 58.33% Loss:0.0107\n",
      "Epoch 21, step/total_step: 40/60 66.67% Loss:0.0086\n",
      "Epoch 21, step/total_step: 45/60 75.00% Loss:0.0069\n",
      "Epoch 21, step/total_step: 50/60 83.33% Loss:0.0083\n",
      "Epoch 21, step/total_step: 55/60 91.67% Loss:0.0066\n",
      "Epoch 21, step/total_step: 60/60 100.00% Loss:0.0047\n",
      "保存模型...\n",
      "Epoch 21, Val Loss:0.1427\n",
      "Epoch 22, step/total_step: 5/60 8.33% Loss:0.0312\n",
      "Epoch 22, step/total_step: 10/60 16.67% Loss:0.0176\n",
      "Epoch 22, step/total_step: 15/60 25.00% Loss:0.0210\n",
      "Epoch 22, step/total_step: 20/60 33.33% Loss:0.0104\n",
      "Epoch 22, step/total_step: 25/60 41.67% Loss:0.0106\n",
      "Epoch 22, step/total_step: 30/60 50.00% Loss:0.0101\n",
      "Epoch 22, step/total_step: 35/60 58.33% Loss:0.0093\n",
      "Epoch 22, step/total_step: 40/60 66.67% Loss:0.0077\n",
      "Epoch 22, step/total_step: 45/60 75.00% Loss:0.0064\n",
      "Epoch 22, step/total_step: 50/60 83.33% Loss:0.0072\n",
      "Epoch 22, step/total_step: 55/60 91.67% Loss:0.0058\n",
      "Epoch 22, step/total_step: 60/60 100.00% Loss:0.0041\n",
      "保存模型...\n",
      "Epoch 22, Val Loss:0.1448\n",
      "Epoch 23, step/total_step: 5/60 8.33% Loss:0.0270\n",
      "Epoch 23, step/total_step: 10/60 16.67% Loss:0.0156\n",
      "Epoch 23, step/total_step: 15/60 25.00% Loss:0.0171\n",
      "Epoch 23, step/total_step: 20/60 33.33% Loss:0.0088\n",
      "Epoch 23, step/total_step: 25/60 41.67% Loss:0.0093\n",
      "Epoch 23, step/total_step: 30/60 50.00% Loss:0.0070\n",
      "Epoch 23, step/total_step: 35/60 58.33% Loss:0.0096\n",
      "Epoch 23, step/total_step: 40/60 66.67% Loss:0.0067\n",
      "Epoch 23, step/total_step: 45/60 75.00% Loss:0.0055\n",
      "Epoch 23, step/total_step: 50/60 83.33% Loss:0.0066\n",
      "Epoch 23, step/total_step: 55/60 91.67% Loss:0.0058\n",
      "Epoch 23, step/total_step: 60/60 100.00% Loss:0.0037\n",
      "保存模型...\n",
      "Epoch 23, Val Loss:0.1468\n",
      "Epoch 24, step/total_step: 5/60 8.33% Loss:0.0250\n",
      "Epoch 24, step/total_step: 10/60 16.67% Loss:0.0138\n",
      "Epoch 24, step/total_step: 15/60 25.00% Loss:0.0156\n",
      "Epoch 24, step/total_step: 20/60 33.33% Loss:0.0073\n",
      "Epoch 24, step/total_step: 25/60 41.67% Loss:0.0078\n",
      "Epoch 24, step/total_step: 30/60 50.00% Loss:0.0056\n",
      "Epoch 24, step/total_step: 35/60 58.33% Loss:0.0073\n",
      "Epoch 24, step/total_step: 40/60 66.67% Loss:0.0065\n",
      "Epoch 24, step/total_step: 45/60 75.00% Loss:0.0048\n",
      "Epoch 24, step/total_step: 50/60 83.33% Loss:0.0055\n",
      "Epoch 24, step/total_step: 55/60 91.67% Loss:0.0045\n",
      "Epoch 24, step/total_step: 60/60 100.00% Loss:0.0032\n",
      "保存模型...\n",
      "Epoch 24, Val Loss:0.1476\n",
      "Epoch 25, step/total_step: 5/60 8.33% Loss:0.0216\n",
      "Epoch 25, step/total_step: 10/60 16.67% Loss:0.0127\n",
      "Epoch 25, step/total_step: 15/60 25.00% Loss:0.0136\n",
      "Epoch 25, step/total_step: 20/60 33.33% Loss:0.0065\n",
      "Epoch 25, step/total_step: 25/60 41.67% Loss:0.0068\n",
      "Epoch 25, step/total_step: 30/60 50.00% Loss:0.0049\n",
      "Epoch 25, step/total_step: 35/60 58.33% Loss:0.0062\n",
      "Epoch 25, step/total_step: 40/60 66.67% Loss:0.0052\n",
      "Epoch 25, step/total_step: 45/60 75.00% Loss:0.0043\n",
      "Epoch 25, step/total_step: 50/60 83.33% Loss:0.0048\n",
      "Epoch 25, step/total_step: 55/60 91.67% Loss:0.0040\n",
      "Epoch 25, step/total_step: 60/60 100.00% Loss:0.0029\n",
      "保存模型...\n",
      "Epoch 25, Val Loss:0.1477\n",
      "Epoch 26, step/total_step: 5/60 8.33% Loss:0.0187\n",
      "Epoch 26, step/total_step: 10/60 16.67% Loss:0.0109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, step/total_step: 15/60 25.00% Loss:0.0117\n",
      "Epoch 26, step/total_step: 20/60 33.33% Loss:0.0053\n",
      "Epoch 26, step/total_step: 25/60 41.67% Loss:0.0061\n",
      "Epoch 26, step/total_step: 30/60 50.00% Loss:0.0042\n",
      "Epoch 26, step/total_step: 35/60 58.33% Loss:0.0052\n",
      "Epoch 26, step/total_step: 40/60 66.67% Loss:0.0045\n",
      "Epoch 26, step/total_step: 45/60 75.00% Loss:0.0040\n",
      "Epoch 26, step/total_step: 50/60 83.33% Loss:0.0042\n",
      "Epoch 26, step/total_step: 55/60 91.67% Loss:0.0035\n",
      "Epoch 26, step/total_step: 60/60 100.00% Loss:0.0026\n",
      "保存模型...\n",
      "Epoch 26, Val Loss:0.1472\n",
      "Epoch 27, step/total_step: 5/60 8.33% Loss:0.0165\n",
      "Epoch 27, step/total_step: 10/60 16.67% Loss:0.0090\n",
      "Epoch 27, step/total_step: 15/60 25.00% Loss:0.0102\n",
      "Epoch 27, step/total_step: 20/60 33.33% Loss:0.0045\n",
      "Epoch 27, step/total_step: 25/60 41.67% Loss:0.0052\n",
      "Epoch 27, step/total_step: 30/60 50.00% Loss:0.0038\n",
      "Epoch 27, step/total_step: 35/60 58.33% Loss:0.0046\n",
      "Epoch 27, step/total_step: 40/60 66.67% Loss:0.0038\n",
      "Epoch 27, step/total_step: 45/60 75.00% Loss:0.0035\n",
      "Epoch 27, step/total_step: 50/60 83.33% Loss:0.0038\n",
      "Epoch 27, step/total_step: 55/60 91.67% Loss:0.0032\n",
      "Epoch 27, step/total_step: 60/60 100.00% Loss:0.0024\n",
      "保存模型...\n",
      "Epoch 27, Val Loss:0.1478\n",
      "Epoch 28, step/total_step: 5/60 8.33% Loss:0.0144\n",
      "Epoch 28, step/total_step: 10/60 16.67% Loss:0.0077\n",
      "Epoch 28, step/total_step: 15/60 25.00% Loss:0.0088\n",
      "Epoch 28, step/total_step: 20/60 33.33% Loss:0.0041\n",
      "Epoch 28, step/total_step: 25/60 41.67% Loss:0.0044\n",
      "Epoch 28, step/total_step: 30/60 50.00% Loss:0.0033\n",
      "Epoch 28, step/total_step: 35/60 58.33% Loss:0.0040\n",
      "Epoch 28, step/total_step: 40/60 66.67% Loss:0.0034\n",
      "Epoch 28, step/total_step: 45/60 75.00% Loss:0.0030\n",
      "Epoch 28, step/total_step: 50/60 83.33% Loss:0.0033\n",
      "Epoch 28, step/total_step: 55/60 91.67% Loss:0.0029\n",
      "Epoch 28, step/total_step: 60/60 100.00% Loss:0.0021\n",
      "保存模型...\n",
      "Epoch 28, Val Loss:0.1503\n",
      "Epoch 29, step/total_step: 5/60 8.33% Loss:0.0126\n",
      "Epoch 29, step/total_step: 10/60 16.67% Loss:0.0069\n",
      "Epoch 29, step/total_step: 15/60 25.00% Loss:0.0077\n",
      "Epoch 29, step/total_step: 20/60 33.33% Loss:0.0037\n",
      "Epoch 29, step/total_step: 25/60 41.67% Loss:0.0039\n",
      "Epoch 29, step/total_step: 30/60 50.00% Loss:0.0029\n",
      "Epoch 29, step/total_step: 35/60 58.33% Loss:0.0035\n",
      "Epoch 29, step/total_step: 40/60 66.67% Loss:0.0031\n",
      "Epoch 29, step/total_step: 45/60 75.00% Loss:0.0026\n",
      "Epoch 29, step/total_step: 50/60 83.33% Loss:0.0030\n",
      "Epoch 29, step/total_step: 55/60 91.67% Loss:0.0026\n",
      "Epoch 29, step/total_step: 60/60 100.00% Loss:0.0019\n",
      "保存模型...\n",
      "Epoch 29, Val Loss:0.1516\n",
      "Epoch 30, step/total_step: 5/60 8.33% Loss:0.0110\n",
      "Epoch 30, step/total_step: 10/60 16.67% Loss:0.0061\n",
      "Epoch 30, step/total_step: 15/60 25.00% Loss:0.0067\n",
      "Epoch 30, step/total_step: 20/60 33.33% Loss:0.0032\n",
      "Epoch 30, step/total_step: 25/60 41.67% Loss:0.0034\n",
      "Epoch 30, step/total_step: 30/60 50.00% Loss:0.0026\n",
      "Epoch 30, step/total_step: 35/60 58.33% Loss:0.0031\n",
      "Epoch 30, step/total_step: 40/60 66.67% Loss:0.0028\n",
      "Epoch 30, step/total_step: 45/60 75.00% Loss:0.0024\n",
      "Epoch 30, step/total_step: 50/60 83.33% Loss:0.0027\n",
      "Epoch 30, step/total_step: 55/60 91.67% Loss:0.0024\n",
      "Epoch 30, step/total_step: 60/60 100.00% Loss:0.0018\n",
      "保存模型...\n",
      "Epoch 30, Val Loss:0.1525\n"
     ]
    }
   ],
   "source": [
    "# 对数据集按照长度进行排序\n",
    "word_lists, tag_lists, _ = sort_by_lengths(train_word_lists, train_tag_lists)\n",
    "dev_word_lists, dev_tag_lists, _ = sort_by_lengths(dev_word_lists, dev_tag_lists)\n",
    "\n",
    "B = batch_size\n",
    "for e in range(1, epoches+1):\n",
    "    step = 0\n",
    "    losses = 0.\n",
    "    for ind in range(0, len(word_lists), B):\n",
    "        batch_sents = word_lists[ind:ind+B]\n",
    "        batch_tags = tag_lists[ind:ind+B]\n",
    "        \n",
    "        model.train()  # 什么时候需要设置模型用于训练？\n",
    "        step += 1\n",
    "        # 准备数据\n",
    "        tensorized_sents, lengths = tensorized(batch_sents, word2id)\n",
    "        tensorized_sents = tensorized_sents.to(device)\n",
    "        targets, lengths = tensorized(batch_tags, tag2id)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward\n",
    "        scores = model(tensorized_sents, lengths)\n",
    "\n",
    "        # 计算损失 更新参数\n",
    "        optimizer.zero_grad()\n",
    "        loss = cal_loss(scores, targets, tag2id).to(device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses += loss.item()\n",
    "\n",
    "        if step % TrainingConfig.print_step == 0:\n",
    "            total_step = (len(word_lists) // B + 1)\n",
    "            print(\"Epoch {}, step/total_step: {}/{} {:.2f}% Loss:{:.4f}\".format(\n",
    "                e, step, total_step,\n",
    "                100. * step / total_step,\n",
    "                losses / print_step\n",
    "            ))\n",
    "            losses = 0.\n",
    "\n",
    "    # 每轮结束测试在验证集上的性能，保存最好的一个\n",
    "    val_loss = validate(dev_word_lists, dev_tag_lists, word2id, tag2id)\n",
    "    print(\"Epoch {}, Val Loss:{:.4f}\".format(e, val_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 待整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilstm_train_and_eval(train_data, dev_data, test_data,\n",
    "                          word2id, tag2id, crf=True, remove_O=False):\n",
    "    train_word_lists, train_tag_lists = train_data\n",
    "    dev_word_lists, dev_tag_lists = dev_data\n",
    "    test_word_lists, test_tag_lists = test_data\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    bilstm_model = BILSTM_Model(vocab_size, out_size, crf=crf)\n",
    "    bilstm_model.train(train_word_lists, train_tag_lists,\n",
    "                       dev_word_lists, dev_tag_lists, word2id, tag2id)\n",
    "\n",
    "    model_name = \"bilstm_crf\" if crf else \"bilstm\"\n",
    "    save_model(bilstm_model, \"./ckpts/\"+model_name+\".pkl\")\n",
    "\n",
    "    print(\"训练完毕,共用时{}秒.\".format(int(time.time()-start)))\n",
    "    print(\"评估{}模型中...\".format(model_name))\n",
    "    pred_tag_lists, test_tag_lists = bilstm_model.test(\n",
    "        test_word_lists, test_tag_lists, word2id, tag2id)\n",
    "\n",
    "    metrics = Metrics(test_tag_lists, pred_tag_lists, remove_O=remove_O)\n",
    "    metrics.report_scores()\n",
    "    metrics.report_confusion_matrix()\n",
    "\n",
    "    return pred_tag_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self, word_lists, tag_lists, word2id, tag2id):\n",
    "    \"\"\"返回最佳模型在测试集上的预测结果\"\"\"\n",
    "    # 准备数据\n",
    "    word_lists, tag_lists, indices = sort_by_lengths(word_lists, tag_lists)\n",
    "    tensorized_sents, lengths = tensorized(word_lists, word2id)\n",
    "    tensorized_sents = tensorized_sents.to(self.device)\n",
    "\n",
    "    self.best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_tagids = self.best_model.test(\n",
    "            tensorized_sents, lengths, tag2id)\n",
    "\n",
    "    # 将id转化为标注\n",
    "    pred_tag_lists = []\n",
    "    id2tag = dict((id_, tag) for tag, id_ in tag2id.items())\n",
    "    for i, ids in enumerate(batch_tagids):\n",
    "        tag_list = []\n",
    "        if self.crf:\n",
    "            for j in range(lengths[i] - 1):  # crf解码过程中，end被舍弃\n",
    "                tag_list.append(id2tag[ids[j].item()])\n",
    "        else:\n",
    "            for j in range(lengths[i]):\n",
    "                tag_list.append(id2tag[ids[j].item()])\n",
    "        pred_tag_lists.append(tag_list)\n",
    "\n",
    "    # indices存有根据长度排序后的索引映射的信息\n",
    "    # 比如若indices = [1, 2, 0] 则说明原先索引为1的元素映射到的新的索引是0，\n",
    "    # 索引为2的元素映射到新的索引是1...\n",
    "    # 下面根据indices将pred_tag_lists和tag_lists转化为原来的顺序\n",
    "    ind_maps = sorted(list(enumerate(indices)), key=lambda e: e[1])\n",
    "    indices, _ = list(zip(*ind_maps))\n",
    "    pred_tag_lists = [pred_tag_lists[i] for i in indices]\n",
    "    tag_lists = [tag_lists[i] for i in indices]\n",
    "\n",
    "    return pred_tag_lists, tag_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_name):\n",
    "    \"\"\"用于保存模型\"\"\"\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n",
    "def load_model(file_name):\n",
    "    \"\"\"用于加载模型\"\"\"\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
