{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 LSTM 进行命名实体识别\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn_crfsuite'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-17312bb6a3bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcodecs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn_crfsuite\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCRF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn_crfsuite'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "from os.path import join\n",
    "from codecs import open\n",
    "\n",
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(split, make_vocab=True, data_dir=\"./ResumeNER\"):\n",
    "    \"\"\"读取数据\"\"\"\n",
    "    assert split in ['train', 'dev', 'test']\n",
    "\n",
    "    word_lists = []\n",
    "    tag_lists = []\n",
    "    with open(join(data_dir, split+\".char.bmes\"), 'r', encoding='utf-8') as f:\n",
    "        word_list = []\n",
    "        tag_list = []\n",
    "        for line in f:\n",
    "            if line != '\\n':\n",
    "                word, tag = line.strip('\\n').split()\n",
    "                word_list.append(word)\n",
    "                tag_list.append(tag)\n",
    "            else:\n",
    "                word_lists.append(word_list)\n",
    "                tag_lists.append(tag_list)\n",
    "                word_list = []\n",
    "                tag_list = []\n",
    "\n",
    "    # 如果make_vocab为True，还需要返回word2id和tag2id\n",
    "    if make_vocab:\n",
    "        word2id = build_map(word_lists)\n",
    "        tag2id = build_map(tag_lists)\n",
    "        return word_lists, tag_lists, word2id, tag2id\n",
    "    else:\n",
    "        return word_lists, tag_lists\n",
    "\n",
    "\n",
    "def build_map(lists):\n",
    "    maps = {}\n",
    "    for list_ in lists:\n",
    "        for e in list_:\n",
    "            if e not in maps:\n",
    "                maps[e] = len(maps)\n",
    "\n",
    "    return maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_lists, train_tag_lists, word2id, tag2id = build_corpus(\"train\")\n",
    "dev_word_lists, dev_tag_lists = build_corpus(\"dev\", make_vocab=False)\n",
    "test_word_lists, test_tag_lists = build_corpus(\"test\", make_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集输入数据： 高勇：男，中国国籍，无境外居留权，\n",
      "训练集标签数据： B-NAME E-NAME O O O B-CONT M-CONT M-CONT E-CONT O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "print(\"训练集输入数据：\", ''.join(train_word_lists[0]))\n",
    "print(\"训练集标签数据：\", ' '.join(train_tag_lists[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_name):\n",
    "    \"\"\"用于保存模型\"\"\"\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n",
    "def load_model(file_name):\n",
    "    \"\"\"用于加载模型\"\"\"\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "def flatten_lists(lists):\n",
    "    flatten_list = []\n",
    "    for l in lists:\n",
    "        if type(l) == list:\n",
    "            flatten_list += l\n",
    "        else:\n",
    "            flatten_list.append(l)\n",
    "    return flatten_list\n",
    "\n",
    "def sent2features(sent):\n",
    "    \"\"\"抽取序列特征\"\"\"\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRFModel(object):\n",
    "    def __init__(self,\n",
    "                 algorithm='lbfgs',\n",
    "                 c1=0.1,\n",
    "                 c2=0.1,\n",
    "                 max_iterations=100,\n",
    "                 all_possible_transitions=False\n",
    "                 ):\n",
    "\n",
    "        self.model = CRF(algorithm=algorithm,\n",
    "                         c1=c1,\n",
    "                         c2=c2,\n",
    "                         max_iterations=max_iterations,\n",
    "                         all_possible_transitions=all_possible_transitions)\n",
    "\n",
    "    def train(self, sentences, tag_lists):\n",
    "        features = [sent2features(s) for s in sentences]\n",
    "        self.model.fit(features, tag_lists)\n",
    "\n",
    "    def test(self, sentences):\n",
    "        features = [sent2features(s) for s in sentences]\n",
    "        pred_tag_lists = self.model.predict(features)\n",
    "        return pred_tag_lists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(object):\n",
    "    \"\"\"用于评价模型，计算每个标签的精确率，召回率，F1分数\"\"\"\n",
    "\n",
    "    def __init__(self, golden_tags, predict_tags, remove_O=False):\n",
    "\n",
    "        # [[t1, t2], [t3, t4]...] --> [t1, t2, t3, t4...]\n",
    "        self.golden_tags = flatten_lists(golden_tags)\n",
    "        self.predict_tags = flatten_lists(predict_tags)\n",
    "\n",
    "        if remove_O:  # 将O标记移除，只关心实体标记\n",
    "            self._remove_Otags()\n",
    "\n",
    "        # 辅助计算的变量\n",
    "        self.tagset = set(self.golden_tags)\n",
    "        self.correct_tags_number = self.count_correct_tags()\n",
    "        self.predict_tags_counter = Counter(self.predict_tags)\n",
    "        self.golden_tags_counter = Counter(self.golden_tags)\n",
    "\n",
    "        # 计算精确率\n",
    "        self.precision_scores = self.cal_precision()\n",
    "\n",
    "        # 计算召回率\n",
    "        self.recall_scores = self.cal_recall()\n",
    "\n",
    "        # 计算F1分数\n",
    "        self.f1_scores = self.cal_f1()\n",
    "\n",
    "    def cal_precision(self):\n",
    "\n",
    "        precision_scores = {}\n",
    "        for tag in self.tagset:\n",
    "            precision_scores[tag] = self.correct_tags_number.get(tag, 0) / \\\n",
    "                self.predict_tags_counter[tag]\n",
    "\n",
    "        return precision_scores\n",
    "\n",
    "    def cal_recall(self):\n",
    "\n",
    "        recall_scores = {}\n",
    "        for tag in self.tagset:\n",
    "            recall_scores[tag] = self.correct_tags_number.get(tag, 0) / \\\n",
    "                self.golden_tags_counter[tag]\n",
    "        return recall_scores\n",
    "\n",
    "    def cal_f1(self):\n",
    "        f1_scores = {}\n",
    "        for tag in self.tagset:\n",
    "            p, r = self.precision_scores[tag], self.recall_scores[tag]\n",
    "            f1_scores[tag] = 2*p*r / (p+r+1e-10)  # 加上一个特别小的数，防止分母为0\n",
    "        return f1_scores\n",
    "\n",
    "    def report_scores(self):\n",
    "        \"\"\"将结果用表格的形式打印出来，像这个样子：\n",
    "\n",
    "                      precision    recall  f1-score   support\n",
    "              B-LOC      0.775     0.757     0.766      1084\n",
    "              I-LOC      0.601     0.631     0.616       325\n",
    "             B-MISC      0.698     0.499     0.582       339\n",
    "             I-MISC      0.644     0.567     0.603       557\n",
    "              B-ORG      0.795     0.801     0.798      1400\n",
    "              I-ORG      0.831     0.773     0.801      1104\n",
    "              B-PER      0.812     0.876     0.843       735\n",
    "              I-PER      0.873     0.931     0.901       634\n",
    "\n",
    "          avg/total      0.779     0.764     0.770      6178\n",
    "        \"\"\"\n",
    "        # 打印表头\n",
    "        header_format = '{:>9s}  {:>9} {:>9} {:>9} {:>9}'\n",
    "        header = ['precision', 'recall', 'f1-score', 'support']\n",
    "        print(header_format.format('', *header))\n",
    "\n",
    "        row_format = '{:>9s}  {:>9.4f} {:>9.4f} {:>9.4f} {:>9}'\n",
    "        # 打印每个标签的 精确率、召回率、f1分数\n",
    "        for tag in self.tagset:\n",
    "            print(row_format.format(\n",
    "                tag,\n",
    "                self.precision_scores[tag],\n",
    "                self.recall_scores[tag],\n",
    "                self.f1_scores[tag],\n",
    "                self.golden_tags_counter[tag]\n",
    "            ))\n",
    "\n",
    "        # 计算并打印平均值\n",
    "        avg_metrics = self._cal_weighted_average()\n",
    "        print(row_format.format(\n",
    "            'avg/total',\n",
    "            avg_metrics['precision'],\n",
    "            avg_metrics['recall'],\n",
    "            avg_metrics['f1_score'],\n",
    "            len(self.golden_tags)\n",
    "        ))\n",
    "\n",
    "    def count_correct_tags(self):\n",
    "        \"\"\"计算每种标签预测正确的个数(对应精确率、召回率计算公式上的tp)，用于后面精确率以及召回率的计算\"\"\"\n",
    "        correct_dict = {}\n",
    "        for gold_tag, predict_tag in zip(self.golden_tags, self.predict_tags):\n",
    "            if gold_tag == predict_tag:\n",
    "                if gold_tag not in correct_dict:\n",
    "                    correct_dict[gold_tag] = 1\n",
    "                else:\n",
    "                    correct_dict[gold_tag] += 1\n",
    "\n",
    "        return correct_dict\n",
    "\n",
    "    def _cal_weighted_average(self):\n",
    "\n",
    "        weighted_average = {}\n",
    "        total = len(self.golden_tags)\n",
    "\n",
    "        # 计算weighted precisions:\n",
    "        weighted_average['precision'] = 0.\n",
    "        weighted_average['recall'] = 0.\n",
    "        weighted_average['f1_score'] = 0.\n",
    "        for tag in self.tagset:\n",
    "            size = self.golden_tags_counter[tag]\n",
    "            weighted_average['precision'] += self.precision_scores[tag] * size\n",
    "            weighted_average['recall'] += self.recall_scores[tag] * size\n",
    "            weighted_average['f1_score'] += self.f1_scores[tag] * size\n",
    "\n",
    "        for metric in weighted_average.keys():\n",
    "            weighted_average[metric] /= total\n",
    "\n",
    "        return weighted_average\n",
    "\n",
    "    def _remove_Otags(self):\n",
    "\n",
    "        length = len(self.golden_tags)\n",
    "        O_tag_indices = [i for i in range(length)\n",
    "                         if self.golden_tags[i] == 'O']\n",
    "\n",
    "        self.golden_tags = [tag for i, tag in enumerate(self.golden_tags)\n",
    "                            if i not in O_tag_indices]\n",
    "\n",
    "        self.predict_tags = [tag for i, tag in enumerate(self.predict_tags)\n",
    "                             if i not in O_tag_indices]\n",
    "        print(\"原总标记数为{}，移除了{}个O标记，占比{:.2f}%\".format(\n",
    "            length,\n",
    "            len(O_tag_indices),\n",
    "            len(O_tag_indices) / length * 100\n",
    "        ))\n",
    "\n",
    "    def report_confusion_matrix(self):\n",
    "        \"\"\"计算混淆矩阵\"\"\"\n",
    "\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        tag_list = list(self.tagset)\n",
    "        # 初始化混淆矩阵 matrix[i][j]表示第i个tag被模型预测成第j个tag的次数\n",
    "        tags_size = len(tag_list)\n",
    "        matrix = []\n",
    "        for i in range(tags_size):\n",
    "            matrix.append([0] * tags_size)\n",
    "\n",
    "        # 遍历tags列表\n",
    "        for golden_tag, predict_tag in zip(self.golden_tags, self.predict_tags):\n",
    "            try:\n",
    "                row = tag_list.index(golden_tag)\n",
    "                col = tag_list.index(predict_tag)\n",
    "                matrix[row][col] += 1\n",
    "            except ValueError:  # 有极少数标记没有出现在golden_tags，但出现在predict_tags，跳过这些标记\n",
    "                continue\n",
    "\n",
    "        # 输出矩阵\n",
    "        row_format_ = '{:>7} ' * (tags_size+1)\n",
    "        print(row_format_.format(\"\", *tag_list))\n",
    "        for i, row in enumerate(matrix):\n",
    "            print(row_format_.format(tag_list[i], *row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crf_train_eval(train_data, test_data, remove_O=False):\n",
    "\n",
    "    # 训练CRF模型\n",
    "    train_word_lists, train_tag_lists = train_data\n",
    "    test_word_lists, test_tag_lists = test_data\n",
    "\n",
    "    crf_model = CRFModel()\n",
    "    crf_model.train(train_word_lists, train_tag_lists)\n",
    "    save_model(crf_model, \"./ckpts/crf.pkl\")\n",
    "\n",
    "    pred_tag_lists = crf_model.test(test_word_lists)\n",
    "\n",
    "    metrics = Metrics(test_tag_lists, pred_tag_lists, remove_O=remove_O)\n",
    "    metrics.report_scores()\n",
    "    metrics.report_confusion_matrix()\n",
    "\n",
    "    return pred_tag_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在训练评估HMM模型...\n",
      "           precision    recall  f1-score   support\n",
      "    E-PRO     0.6512    0.8485    0.7368        33\n",
      "    E-EDU     0.9167    0.9821    0.9483       112\n",
      "    B-EDU     0.9000    0.9643    0.9310       112\n",
      "    M-EDU     0.9348    0.9609    0.9477       179\n",
      "   E-RACE     1.0000    0.9286    0.9630        14\n",
      "   E-CONT     0.9655    1.0000    0.9825        28\n",
      "  E-TITLE     0.9514    0.9637    0.9575       772\n",
      "    B-PRO     0.5581    0.7273    0.6316        33\n",
      "   B-CONT     0.9655    1.0000    0.9825        28\n",
      "    E-LOC     0.5000    0.5000    0.5000         6\n",
      "    B-LOC     0.3333    0.3333    0.3333         6\n",
      "    M-PRO     0.4490    0.6471    0.5301        68\n",
      "   B-RACE     1.0000    0.9286    0.9630        14\n",
      "   B-NAME     0.9800    0.8750    0.9245       112\n",
      "    M-LOC     0.5833    0.3333    0.4242        21\n",
      "    B-ORG     0.8422    0.8879    0.8644       553\n",
      "  B-TITLE     0.8811    0.8925    0.8867       772\n",
      "   M-NAME     0.9459    0.8537    0.8974        82\n",
      "        O     0.9568    0.9177    0.9369      5190\n",
      "   M-CONT     0.9815    1.0000    0.9907        53\n",
      "    E-ORG     0.8262    0.8680    0.8466       553\n",
      "  M-TITLE     0.9038    0.8751    0.8892      1922\n",
      "   E-NAME     0.9000    0.8036    0.8491       112\n",
      "    M-ORG     0.9002    0.9327    0.9162      4325\n",
      "avg/total     0.9149    0.9122    0.9130     15100\n",
      "\n",
      "Confusion Matrix:\n",
      "          E-PRO   E-EDU   B-EDU   M-EDU  E-RACE  E-CONT E-TITLE   B-PRO  B-CONT   E-LOC   B-LOC   M-PRO  B-RACE  B-NAME   M-LOC   B-ORG B-TITLE  M-NAME       O  M-CONT   E-ORG M-TITLE  E-NAME   M-ORG \n",
      "  E-PRO      28       1       1       1       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       2       0       0       0 \n",
      "  E-EDU       1     110       0       1       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n",
      "  B-EDU       0       0     108       3       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       1 \n",
      "  M-EDU       0       0       0     172       0       0       0       1       0       0       0       4       0       0       0       0       0       0       1       0       1       0       0       0 \n",
      " E-RACE       0       0       0       0      13       0       0       0       0       0       0       0       0       0       0       0       0       0       1       0       0       0       0       0 \n",
      " E-CONT       0       0       0       0       0      28       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n",
      "E-TITLE       0       1       0       0       0       0     744       0       0       0       0       0       0       0       0       4       0       0       6       0       0       2       0      15 \n",
      "  B-PRO       0       0       1       0       0       0       0      24       0       0       0       3       0       0       0       0       0       0       0       0       0       0       0       5 \n",
      " B-CONT       0       0       0       0       0       0       0       0      28       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n",
      "  E-LOC       0       0       0       0       0       0       0       0       0       3       0       0       0       0       0       0       0       0       2       0       0       0       0       1 \n",
      "  B-LOC       0       0       0       0       0       0       0       0       0       0       2       0       0       0       0       3       0       0       1       0       0       0       0       0 \n",
      "  M-PRO       0       0       1       1       0       0       0       1       0       0       0      44       0       0       0       0       0       0       0       0       3       0       0      18 \n",
      " B-RACE       0       0       0       0       0       0       0       0       0       0       0       0      13       0       0       0       0       0       1       0       0       0       0       0 \n",
      " B-NAME       0       0       0       0       0       0       0       0       0       0       0       0       0      98       0       1       0       0       8       0       0       0       0       2 \n",
      "  M-LOC       0       0       0       0       0       0       0       0       0       0       1       0       0       0       7       0       0       0       4       0       2       0       0       7 \n",
      "  B-ORG       0       0       0       0       0       0       0       0       1       0       3       0       0       1       0     491       6       0      28       0       0       0       0      23 \n",
      "B-TITLE       0       0       2       0       0       0       1       2       0       0       0       0       0       0       0       6     689       0      20       0       1      28       0      23 \n",
      " M-NAME       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      70       3       0       0       0       6       3 \n",
      "      O       4       2       1       1       0       0      26       3       0       0       0      12       0       0       0      37      26       0    4763       0      30      78       2     204 \n",
      " M-CONT       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      53       0       0       0       0 \n",
      "  E-ORG       0       0       1       0       0       0       1       1       0       0       0       3       0       0       0       0       9       0      10       0     480      18       0      30 \n",
      "M-TITLE       3       3       2       4       0       0       6       1       0       0       0       7       0       0       0       3      35       0      44       0      17    1682       0     115 \n",
      " E-NAME       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       2      16       0       3       0      90       0 \n",
      "  M-ORG       7       3       3       1       0       1       4      10       0       3       0      25       0       1       5      38      17       2      70       1      42      53       2    4034 \n"
     ]
    }
   ],
   "source": [
    "# 训练评估CRF模型\n",
    "    print(\"正在训练评估CRF模型...\")\n",
    "    crf_pred = crf_train_eval(\n",
    "        (train_word_lists, train_tag_lists),\n",
    "        (test_word_lists, test_tag_lists)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
